{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Original network adapted from karpathy\n",
    "# minesh.mathew@gmail.com\n",
    "# modified version of text generation example in keras;\n",
    "# trained in a many-to-many fashion using a time distributed dense layer\n",
    "\n",
    "####\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import matplotlib as mptl\n",
    "import pylab\n",
    "\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LeakyReLU, BatchNormalization\n",
    "# from keras.layers import LSTM, TimeDistributedDense, SimpleRNN  #DEPRECATED TimeDistributedDense\n",
    "from keras.layers import LSTM, TimeDistributed, SimpleRNN, CuDNNGRU, CuDNNLSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "keras.backend.tensorflow_backend.set_session(sess)\n",
    "#import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------- TEXT FILE PREPROCESSING -----------------\n",
    "\n",
    "def preprocess_text_file(filename, maxlen=40):\n",
    "    \"\"\"load a file and split the text it contains into sequences of length = maxlen\n",
    "    returns text, chars, char_indices, indices_char, sentences, next_chars\n",
    "    text: the raw text (turned into lowercase)\n",
    "    chars: a list of unique characters in the text\n",
    "    char_indices: a dictionary of the character-to-index conversion\n",
    "    indices_char: a dictionary of the index-to-character conversion\n",
    "    sequences: a list of the sequences of max length extracted from the file (stride specified by the step variable below)\n",
    "    next_chars: a list of the corresponding sequences of max_length next-characters following each of the sequence character members\n",
    "      in other words, each member of next_chars is contains the last maxlen-1 chars of the correspondiong sequence ...\n",
    "      and the next character from the text after the last character in that sequence \"\"\"\n",
    "    print('loading: ', filename)\n",
    "    text = open(filename).read().lower()\n",
    "    print('corpus length:', len(text))\n",
    "    chars = sorted(list(set(text)- set([\"\\n\"])))   #returns unique characters from the text\n",
    "    print('total chars:', len(chars))\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    text = None\n",
    "    # split the corpus into sequences of length=maxlen\n",
    "    # input is a sequence of 40 chars and target is also a sequence of 40 chars shifted by one position\n",
    "    # for eg: if you maxlen=3 and the text corpus is abcdefghi, your input ---> target pairs will be\n",
    "    # [a,b,c] --> [b,c,d], [b,c,d]--->[c,d,e]....and so on\n",
    "    step = 1\n",
    "    sequences = []\n",
    "    next_chars = []\n",
    "    tweets = open(filename).readlines()\n",
    "    #first generate sentences of characters\n",
    "    for text in tweets:\n",
    "        text = text.lower().strip()\n",
    "        for i in range(0, len(text) - maxlen + 1, step):\n",
    "            sequences.append(text[i: i + maxlen])  # input seq is from i to i  + maxlen\n",
    "            next_chars.append(text[i + 1:i + 1 + maxlen])  # output seq is from i+1 to i+1+maxlen\n",
    "    print('number of sequences:', len(sequences))\n",
    "    return text, chars, char_indices, indices_char, sequences, next_chars\n",
    "\n",
    "\n",
    "def vectorize_text(chars, char_indices, sentences, next_chars, maxlen=40):  #UNUSED: \"next_chars\"\n",
    "    \"\"\"Accepts a list of sentences to convert to indices.  Used characters, their corresponding indeces to produce a set of sequences\n",
    "    of X and corresponding labels y\"\"\"\n",
    "    # now generate dummy variables (1-hot vectors) for the sequences of characters\n",
    "    print('Vectorization processing... this could take a while...')\n",
    "    X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), maxlen, len(chars)),\n",
    "                 dtype=np.bool)  # y is also a sequence , or  a seq of 1 hot vectors\n",
    "    joblength = len(sentences)\n",
    "    tenpercent = joblength/10\n",
    "    nextpercent = tenpercent\n",
    "    print(\" part 1 of 2\")\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if i>nextpercent:\n",
    "            print(i, \" of \", joblength, \" completed\")\n",
    "            nextpercent += tenpercent\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1  # X has dimension [sentence_count, sentence_length, char_count]\n",
    "    print(\" part 2 of 2\")\n",
    "    nextpercent = tenpercent\n",
    "\n",
    "    for i, sentence in enumerate(next_chars):\n",
    "        if i>nextpercent:\n",
    "            print(i, \" of \", joblength, \" completed\")\n",
    "            nextpercent += tenpercent\n",
    "        for t, char in enumerate(sentence):\n",
    "            y[i, t, char_indices[char]] = 1  # y has dimension [sentence_count, sentence_length, char_count]\n",
    "    print('vetorization completed')\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def generate_text(model, char_indices, indices_char, seed_string=\"brutus:\", generate_character_count=320):\n",
    "    \"\"\"Generates text using a model\"\"\"\n",
    "    print(\"seed string --> \", seed_string)\n",
    "    print('The generated text is: ')\n",
    "    sys.stdout.write(seed_string),\n",
    "    # x=np.zeros((1, len(seed_string), len(chars)))\n",
    "    for i in range(generate_character_count):\n",
    "        x = np.zeros((1, len(seed_string), len(chars)))\n",
    "        for t, char in enumerate(seed_string):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        # print (np.argmax(preds[7]))\n",
    "        next_index = np.argmax(preds[len(seed_string) - 1])\n",
    "\n",
    "        # next_index=np.argmax(preds[len(seed_string)-11])\n",
    "        # print (preds.shape)\n",
    "        # print (preds)\n",
    "        # next_index = sample(preds, 1) #diversity is 1\n",
    "        next_char = indices_char[next_index]\n",
    "        seed_string = seed_string + next_char\n",
    "\n",
    "        # print (seed_string)\n",
    "        # print ('##############')\n",
    "        # if i==40:\n",
    "        #    print ('####')\n",
    "        sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "    return seed_string\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- MODEL FILE I/O ---------------------------\n",
    "def save_model(model, save_dir=os.path.join(os.getcwd(), 'saved_models'),\n",
    "               model_file_name='keras_trumptweets_trained_model.h5'):\n",
    "    \"\"\"\n",
    "    Save model and current weights\n",
    "    :param model: Keras model\n",
    "    :param save_dir: path name to save directory\n",
    "    :param model_file_name: filename for saved model\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_file_name)\n",
    "    model.save(model_path)\n",
    "    print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "\n",
    "def load_model(save_dir, model_file_name):\n",
    "    # Load model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_file_name)\n",
    "    model = keras.models.load_model(model_path)\n",
    "    print('Loaded trained model from %s ' % model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------- MODEL ARCHITECTURE ---------------------------\n",
    "def build_model(characters):\n",
    "    # build the model: 2 stacked LSTM\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(1024, return_sequences=True, input_shape=(None,len(characters)) ))  # minesh witout specifying the input_length\n",
    "    model.add(CuDNNLSTM(1024, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation=\"relu\", bias_initializer=keras.initializers.Constant(value=0.01))) \n",
    "    model.add(TimeDistributed(Dense(len(characters)))) \n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    print('model is made')\n",
    "    # train the model, output generated text after each iteration\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- MODEL TRAINING ---------------------------\n",
    "def train_net(model, x, y, training_iterations=25, maxlen=40, save_all_model_iterations=True):\n",
    "    for training_iteration in range(1, training_iterations+1):\n",
    "        print()\n",
    "        print('-' * 50)\n",
    "        print('Training Iteration (epoch) #:', training_iteration)\n",
    "        history = model.fit(x, y, batch_size=512, epochs=1, verbose=1)    #train 1 epoch at a time using previous weights\n",
    "        sleep(0.1)  # https://github.com/fchollet/keras/issues/2110\n",
    "\n",
    "        # saving models at the following iterations -- uncomment it if you want tos save weights and load it later\n",
    "        # if training_iteration==1 or training_iteration==3 or training_iteration==5 or training_iteration==10 or training_iteration==20 or training_iteration==30 or training_iteration==50 or training_iteration==60 :\n",
    "\n",
    "        # # save every training_iteration of weights\n",
    "        # model.save_weights('Karpathy_LSTM_weights_' + str(training_iteration) + '.h5', overwrite=True)\n",
    "        # start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "        save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "        current_model_file_name = 'Trump_LSTM_model_' + str(training_iteration) + '.h5'\n",
    "        if save_all_model_iterations:\n",
    "            save_model(model=model, save_dir=save_dir, model_file_name=current_model_file_name)\n",
    "        sys.stdout.flush()\n",
    "        print('loss is')\n",
    "        print(history.history['loss'][0])\n",
    "        print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading:  ./textdatasets/trumptweets.txt\n",
      "corpus length: 2264736\n",
      "total chars: 86\n",
      "number of sequences: 1496335\n",
      "Vectorization processing... this could take a while...\n",
      " part 1 of 2\n",
      "149634  of  1496335  completed\n",
      "299268  of  1496335  completed\n",
      "448901  of  1496335  completed\n",
      "598535  of  1496335  completed\n",
      "748168  of  1496335  completed\n",
      "897802  of  1496335  completed\n",
      "1047435  of  1496335  completed\n",
      "1197069  of  1496335  completed\n",
      "1346702  of  1496335  completed\n",
      " part 2 of 2\n",
      "149634  of  1496335  completed\n",
      "299268  of  1496335  completed\n",
      "448901  of  1496335  completed\n",
      "598535  of  1496335  completed\n",
      "748168  of  1496335  completed\n",
      "897802  of  1496335  completed\n",
      "1047435  of  1496335  completed\n",
      "1197069  of  1496335  completed\n",
      "1346702  of  1496335  completed\n",
      "vetorization completed\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "#--------------------- Main Code -----------------------------\n",
    "\n",
    "\n",
    "# pick the filename you want to use, and comment out the rest\n",
    "# make sure you have this directory structure\n",
    "raw_text_filename='./textdatasets/trumptweets.txt'\n",
    "    \n",
    "\n",
    "text, chars, char_indices, indices_char, sentences, next_chars = preprocess_text_file(raw_text_filename)\n",
    "\n",
    "#vectorized form takes too much space to save... so process in real time\n",
    "X, y = vectorize_text(chars, char_indices, sentences, next_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Andrew\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Andrew\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Andrew\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Andrew\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Loaded trained model from C:\\Users\\Andrew\\Desktop\\trumpbot\\saved_models\\Trump_LSTM_model_25.h5 \n"
     ]
    }
   ],
   "source": [
    "TRAIN_MODE = False\n",
    "if TRAIN_MODE:\n",
    "    model = build_model(characters=chars)\n",
    "    model_epoch_training_iterations = 25    #the bigger your text corpus, the smaller you can make this\n",
    "    model = train_net(model=model, x=X, y=y,\n",
    "                      training_iterations=model_epoch_training_iterations,\n",
    "                      save_all_model_iterations=True)\n",
    "else:  # load a model from a file\n",
    "    # decide which iteration of the trained model you want to explore\n",
    "    model_training_iteration = 25\n",
    "    save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "    current_model_file_name = 'Trump_LSTM_model_' + str(model_training_iteration) + '.h5'\n",
    "    model = load_model(save_dir=save_dir, model_file_name=current_model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed string -->  covfefe\n",
      "The generated text is: \n",
      "covfefering the democrat debate live on twitter! lets do this!!! @realdonaldtrump #1 • with a lead 2x's higher than #2 jeb bush. #trump2016?"
     ]
    }
   ],
   "source": [
    "GENERATE_TEXT_MODE = True\n",
    "if GENERATE_TEXT_MODE:   # generate text mode\n",
    "    #decide which saved model to load\n",
    "    #make up a string of characters to start with\n",
    "    seed_string = \"covfefe\" # pick something from the news\n",
    "    # decide how many text characters you want to generate:\n",
    "    gen_char_count = 140 - len(seed_string) # has to be a tweet!\n",
    "    tweet = generate_text(model, char_indices, indices_char, seed_string, generate_character_count=gen_char_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
